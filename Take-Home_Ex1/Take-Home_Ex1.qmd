---
title: "Take Home Exercise 1 - Investigation of Water points in Nigeria"
author: "Allan Chong"
editor: visual
execute: 
  warning: false
  message: false
---

## Overview

Water is a crucial resource for humanity. People must have access to clean water in order to be healthy. It promotes a healthy environment, peace and security, and a sustainable economy. However, more than 40% of the world's population lacks access to enough clean water. According to UN-Water, 1.8 billion people would live in places with a complete water shortage by 2025. One of the many areas that the water problem gravely threatens is food security. Agriculture uses over 70% of the freshwater that is present on Earth.

The severe water shortages and water quality issues are seen in underdeveloped countries. Up to 80% of infections in developing nations are attributed to inadequate water and sanitation infrastructure.

Despite technological advancement, providing rural people with clean water continues to be a key development concern in many countries around the world, especially in those on the continent of Africa.

The spatial patterns of non-functional water points will be shown in this study by using the proper global and local spatial association methodologies. We look at Nigeria's in this assignment.

## Getting Started

First, the required packages are loaded into the R environment . The required packages are **sf,** **tidyverse**, **spdep**, **tmap**, & **funModeling**

with the code below:

```{r}
pacman::p_load(sf, tidyverse, spdep, tmap, funModeling)
```

### Spatial Data

The spatial dataset used in this assignment is the Nigeria Level-2 Administrative Boundary spatial dataset downloaded from Center for Humanitarian Data - [Nigeria - Subnational Administrative Boundaries](https://data.humdata.org/dataset/cod-ab-nga)

We will load the spatial features by using `st_read()` from the **sf** package

As the data we want is in WSG-84 format, we set *crs* to 4326.

We won't utilize st transform() at this time because it can result in outputs with missing points after transformation, which would skew our study.

```{r}
nga = st_read(dsn = "data/geospatial",
               layer = "nga_admbnda_adm2_osgof_20190417",
               crs = 4326)

#nigeria_lga_sf = st_transform(nigeria_lga_sf, crs=4326) cause missing points

```

We could use `st_crs()`to verify the coordinate system from the object.

```{r}
st_crs(nga)
```

Before we start analyzing the data, lets us take a look at some characteristics of the spatial features to have a sense of what we are dealing with. We can use *`glimpse()`* to determine to accomplish that

```{r}
glimpse(nga)
```

We can use `freq()` of the **funModeling** package to display the distribution of Level 1 administration (Which are states in Nigeria) instead and only zooming in on the micro level when we perform the water point analysis.

```{r}
freq(data=nga, input = 'ADM1_EN')
```

774 Local Government Areas (LGA) make up Nigeria's 37 states, with Kano having the most LGAs overall.

For a meaningful analysis, there are just too many LGAs, both large and little.

Calling `ttm()` in the **tmap** package will switch the tmap's viewing mode to interactive viewing, which will help us better visualize the map. Without this change, the tmap will be too small for any type of analysis. Additionally, we'll base the map's plot on States (Level 1 Administration Area)

Given that there are 37 states, we must raise the maximum number of categories from the default value of 30 to 37. Using `tmap_options(max.categories = 37)`, the threshold can be set.

```{r}
ttm()
tmap_options(max.categories = 37)
```

Now, we are ready to build our map with the functions in the **tmap** package

```{r}

tm_shape(nga) + 
 
  tm_polygons("ADM1_EN") +
  tm_borders(alpha=0.5) + 
  tm_scale_bar() +
  tm_grid (alpha=0.2) +
  tm_layout(main.title="Map of Nigeria LGA", 
            main.title.position="center", 
            main.title.size=1.2, 
            legend.height = 0.35, 
            legend.width = 0.35, 
            frame = TRUE) 
```

### Aspatial Data

#### Cleaning the Data

The aspatial dataset used in this assignment is the water point data exchange dataset found in [WPdx Global Data Repositories](https://www.waterpointdata.org/access-data/). Data is filtered on the web portal to only keep Nigeria and the file is saved as *NigeriaWaterPoints_Raw.csv*

As we are only interested in the functionality of the water point, it is important to capture fields that may affect the functionality

-   LGA: The area we are interested in

-   State: The state of the LGA of Nigeria

-   Functional: Whether it is functional or not

-   management: who manages it?

-   Quality: what is the quality?

-   Water Source Category: where the water came from?

-   Water Tech Category: What technology is used?

-   latitude

-   longitude

To load the raw data file, we use the `read_csv` function

```{r}
#| eval: false
wpdx_raw = read_csv("data/aspatial/NigeriaWaterPoints_Raw.csv") 
```

Most of the columns are irrelevant, so we will perform the following:

-   keep the columns we want to clean it up by specifying the columns with one to retain with `subset`

-   renaming the columns using `rename_with`

-   Replace all the NA with unknown for columns with NA value present

```{r}
#| eval: false
retain_cols <- c('#clean_adm2', '#clean_adm1', '#status_clean', '#management_clean', '#subjective_quality', '#fecal_coliform_presence', '#water_source_category', '#water_tech_category', '#lat_deg', '#lon_deg' )

new_col_names <- c('LGA', 'State', 'Functional', 'Management', 'Quality', 'presence_of_fecal_coliform', 'Water_Source_Category', 'Water_Tech_Category', 'latitude', 'longitude')

wpdx_clean = subset(wpdx_raw, select = (names(wpdx_raw) %in% retain_cols)) %>%  rename_with(~ new_col_names, all_of(retain_cols)) %>% 
replace_na(list(Functional = "Unknown", Management = "Unknown", Quality = "Unknown", Water_Source_Category = "Unknown", Water_Tech_Category = "Unknown"))



```

We save the clean file with `saveRDS()`, the file will be reduced to 1.6MB from the 144MB raw file that we downloaded.

```{r}
#| eval: false
saveRDS(wpdx_clean, "data/aspatial/wpdx_clean.rds")
```

We can then delete the raw file from the project and retrieve the saved RDS file using `readRDS()`

```{r}
wpdx_clean = readRDS("data/aspatial/wpdx_clean.rds")

```

#### Converting csv data into spatial features

We can use `st_as_sf`to create a dataframe from the longitude (*x*) and latitude (*y*) values. The EPSG 4326 code is used as the dataset is referencing WGS84 geographic coordinate system. We could use `st_crs()`to verify the coordinate system from the object.

```{r}
wpdx_clean_sf = st_as_sf(wpdx_clean, coords = c("longitude", "latitude"), crs=4326)
st_crs(wpdx_clean_sf)
```

We can then use *`glimpse()`* to verify each field's data type & available values.

There are 95, 008 water points in the LGAs. The results also shows that the longitude and latitude values have been converted to a geometry object consisting of the longitude and latitude values as points, with both columns now dropped.

```{r}
glimpse(wpdx_clean_sf)
```

#### Aggregate the Data

We can use `freq()` of the **funModeling** package to display the distribution of *functional* field in *wpdx_clean_sf*. This is to help us aggregate the data as the dataset provide breakdowns of functional status. In order to only look at non functional water points, we will need to aggregate the different categories into simply functional, non functional and unknowns.

```{r}
freq(data=wpdx_clean_sf, input = 'Functional')
```

To aggregate them into functional, non functional and unknown, we will create new data frames to store them by using the `filter` function

```{r}
func_list = c("Functional", "Functional but needs repair", "Functional but not in use")
wpt_functional = wpdx_clean_sf %>%
  filter(Functional %in% func_list)

wpt_non_functional = wpdx_clean_sf %>%
  filter(!Functional %in% c(func_list, "Unknown"))

wpt_unknown = wpdx_clean_sf %>%
  filter(Functional %in% "Unknown")
```

Out of the 32, 204, records, we can gain some insights on why it might be non functional, is it due to management? Is it due to technology? Is it due to the source of the water?

Similarly, like how we aggregate functional data points, we could use `freq()` of the **funModeling** package to find out

```{r}
freq(data=wpt_non_functional, input = 'Management')
```

```{r}
freq(data=wpt_non_functional, input = 'Water_Tech_Category')
```

```{r}
freq(data=wpt_non_functional, input = 'Water_Source_Category')
```

From the results, we can conclude that

-   More than half of the non functional water points have an unknown management, we could ask if these water points are even managed.

-   Most of the non functional water points uses pumps, we could ask the question if there is an issue with the pumps and if there is a lack of expertise to repair or replace them when they fail.

-   97.72% of such non functional water points are made up of wells.

## Combining Spatial & Aspatial Data

We can use `st_intersects()` to find common data points between geographical datasets. In our case we need to find the common points in the Nigeria's LGA spatial dataset and the water point aspatial dataset

The below code does 4 things

1.  It intersects the Nigeria LGA dataset (*nga* dataframe) with the water point dataset (*wpdx_clean_sf* dataframe) and produce a new column to denote the total number of water points in the area (*Total wpt*) by using `mutate()` and `lengths()`

2.  Similar to step 1, the result of step 1 is piped to add 3 columns to denote the number of functional, non functional and unknown water points in the area to produce *wpt functional*, *wpt non functional* and *wpt unknown* respectively

3.  We also add 2 new columns to find the percentage of functional and non functional water points by using `mutate()`

4.  Select appropriate columns required which are the LGA area and LGA code (Column 3 & 4), Administration Level 1 Area and Administration Level 1 Code (Column 8 & 9) which represent states, the columns that were added as explained in steps 2 & 3 and the geometry multipolygon objects (Column 18 to 23) using `select()`

```{r}
nga_wp <- nga %>% 
  #combine nga with water point sf
  mutate(`total wpt` = lengths(
    st_intersects(nga, wpdx_clean_sf))) %>%
  #add columns to produce no. of functional, non functional and unknown points
  mutate(`wpt functional` = lengths(
    st_intersects(nga, wpt_functional))) %>%
  mutate(`wpt non functional` = lengths(
    st_intersects(nga, wpt_non_functional))) %>%
  mutate(`wpt unknown` = lengths(
    st_intersects(nga, wpt_unknown))) %>%
  
  #add columns to compute %
  mutate(pct_functional = `wpt functional`/`total wpt`) %>%
  mutate(`pct_non-functional` = `wpt non functional`/`total wpt`) %>%
  select(3:4, 8:9, 18:23)
```

## Visualizing the spatial distribution of water points

We could use breaks of the summary statistics by using percentiles, this is to help us find out the distribution of water points in each quantile.

```{r}
#summary(nga_wp$`total wpt`)
#summary(nga_wp$`wpt functional`)
summary(nga_wp$`wpt non functional`)
#summary(nga_wp$`wpt unknown`)
```

It is recommended not to use the default style with breaks as quantile since the range from the third quantile to the maximum is too wide and could result in a skewed representation. We compute the variance and standard deviation of non-functional water points first to better understand our dataset since we now need to decide which style is appropriate for the map.

```{r}
var(nga_wp$`wpt non functional`)
sd(nga_wp$`wpt non functional`) 
```

It appears that this dataset has a very large variance.  Since the variance is so high, we would like to lower it. Using the *kmeans* style is one method to do this. *n = 6* is choosen as after some experimentation, it appears that 6 is the optimal number of clusters.

Functions from the **tmap** packages is used to produce the map

First we use `tm_shape()` `+ tm_fill("ADM1_EN")` to form Layer 1 of the map to form the 37 states of the map. The *Pastel1* palette is used because it is difficult to read different shades of the same two to three colors; Pastel1 has more colors, making states more distinct.

Next we use `tm_shape()` `+ tm_fill("wpt non functional")` to form Layer 2 of the map which are the non functional water points. The palette used in this case is Purple Red such that areas with very little water points are shaded with a very light colour.

We may switch between layers on the interactive map to superimpose the nonfunctional water locations. With so many polygons, putting it side by side can be challenging to interpret.

```{r}
tm_shape(nga) + 
 
  tm_fill("ADM1_EN", palette = "Pastel1") +
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +

  tm_shape(nga_wp) + 
  tm_fill("wpt non functional", 
          palette ="PuRd", style="kmeans", n=6) +  
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +
  tm_layout(main.title="non functional WP - 2 Layer map", 
            main.title.position="center", 
            main.title.size=1.2, 
            #legend.height = 0.35, 
            #legend.width = 0.35, 
            frame = TRUE) 


```

Using **dplyr** package, we can summarize find out which States has the most number non functional water points and which are the states that has the most number of LGAs by using the functions `group_by`, `summarise` and `arrange`

```{r}
#Sum of non functional water points 
nga_wp %>% 
  group_by(ADM1_EN) %>% 
   summarise(NF_Frequency = sum(`wpt non functional`), 
             #F_Frequency = sum(`wpt functional`),
             Total_Freq = sum(`total wpt`),
             NF_Ratio = (NF_Frequency / Total_Freq) * 100
             ) %>% 
    arrange(desc(NF_Frequency))

#sum of LGAs by states 
nga_wp %>% 
  group_by(ADM1_EN) %>% 
  summarise(count = n())%>% 
    arrange(desc(count))
```

### Observations

-   According to the statistics, Osun has the highest number of non-operational water points - 2118 of them among the 37 states, followed by Kaduna (1912 water points) and Kwara (1634 water points).

-   Kano, despite being the State with the most number of LGAs (44), has only 1570 non functional water points (Ranked 4th) as compared to Osun that only comprises of 30 LGAs (Ranked 1st).

-   In contrast to Kaduna & Kwara, which are greater in size, Osun has 5519 water points, which is an interesting fact. In addition, nearly half of the water points in Kwara are not working.

-   Despite having a larger territory, Ondo, the state directly south-east of Osun, has over 60% of its water points that are not operational.

-   The south-eastern and western regions of Nigeria appear to be the hotspots for the spread of inoperative water points.

-   There are no non-functional water points on Nigeria's north-eastern coast. Using the tmap package, we plot the functional map to see if there are any water points in the region or if there are none at all.

    This can assist us in figuring out whether the region in the north-east is succeeding in a way that can be transferred to other parts of the nation, or whether it is uninhabited or underdeveloped.

```{r}
  tm_shape(nga_wp) + 
  tm_fill("wpt functional", 
          palette ="PuRd", style="kmeans", n=6) +  
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +
  tm_layout(main.title="non functional WP - 2 Layer map", 
            main.title.position="center", 
            main.title.size=1.2, 
            #legend.height = 0.35, 
            #legend.width = 0.35, 
            frame = TRUE) 
```

The north-eastern region of Nigeria has few to no water points, which suggests to us that it may be that the region is underdeveloped or uninhabited.

## Spatially Constrained Cluster Analysis

### Computing Spatial Weights

We need to find the spatial weights first before we can compute global spatial correlation statistics. The spatial weights is used to define the neighbourhood relationships between the geographical units

We use `poly2nb()` of **spdep** package to compute the contiguity weight matrix. The function builds a neighbour list based on regions with contiguous boundaries. Using queen\'s contiguity weight matrix, we have

```{r}
wm_q = poly2nb(nga_wp)
summary(wm_q)
```

From the results, there are 774 regions in Hunan,

Using the Queen\'s method, 1 of them has 14 neighbours, 1 of them is an isolate (no neighbour), and 2 of them only has 1 neighbour

### Building the weights matrix

After computing the spatial weights, we will need to build the weights matrix. From the result, of the neighbour list, there exist an isolated island. With this island, it is not recommended to use contiguity weight matrix as we want to find out if there is spatial correlation for non functional water points.

With that in mind, we will use distance based weight matrix instead.

By using `dnearneigh()` of **spdep** package, we can determine the distance based weight matrix. The function looks for neighbours of regions points by Euclidean distance between the *lower (\>=) and upper (\<=) bound* or with the parameter `longlat = True` by great circle distance in km

### Building the distance based weight matrix

#### Fixed Weight distance matrix

The steps involved:

1.  Using the k nearest neighbour (knn) algorithm, we can return a matrix with indices of points that belongs to the set of k nearest neighbours of each others by using `knearneigh()` of **spdep**

2.  Convert the knn objects into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using `knn2nb()`

3.  Return the length of neighbour relationship edges by using `nbdists()` of **spdep**. The function returns in the units of coordinates if the coordinates are projected, in km otherwise.

4.  Remove the list structure of the return objects by using `unlist()`

Before we can start building the matrix, we first need to find the coordinates

-   The longitude is the first variable in each centroid, this enables us to obtain only the longitude.

-   The latitude is the second variable in each centroid, this enables us to obtain only the latitude

Using the double bracket notation \[\[\]\] and the index, we can access the latitude & longitude values.

```{r}
longitude = map_dbl(nga_wp$geometry, ~st_centroid(.x)[[1]]) #longitude index 1
latitude = map_dbl(nga_wp$geometry, ~st_centroid(.x)[[2]]) #latitude index 2
```

After getting the longitude and latitudes, we can form the coordinates object named `coord` using `cbind`.Using the `head` function, we can inspect the elements of `coord` to verify if they are correctly formatted

```{r}
coord = cbind(longitude, latitude)
head(coord)
```

##### Find the lower and upper bounds

```{r}
k1 = knn2nb(knearneigh(coord)) #returns a list of nb objects from the result of k nearest neighbours matrix, Step 1 & 2
k1dist = unlist(nbdists(k1, coord, longlat = TRUE)) #return the length of neighbour relationship edges and remove the list structures, Step 3 & 4
summary(k1dist)
```

From the result, the largest first nearest neighbour is 71.66 km, hence by using this as the upper bound, we can be certain that all units will have at least 1 neighbour

##### Creating the fixed distanced weight matrix

`dnearneigh` will be used to compute the distance weight matrix

```{r}
wm_d72 = dnearneigh(coord, 0, 72, longlat = TRUE)
wm_d72
```

The average number of links denotes the number of non zero links divided by the number of regions. In this case, a region has about on average between 24 neighbours

We will display the structure of the weight matrix is to combine `table()` and `card()` of spdep.

-   The `card()` function counts the neighboring regions in the neighbours list.

-   `table()` creates a contingency table of the counts for each combination of factor levels using cross-classifying factors.

```{r}
table(nga_wp$ADM2_EN, card(wm_d72))
```

To get the number of disconnected connected subgraphs in the wmd72weight matrix, we can use the `n.comp.nb()` function of the **spdep** package.

```{r}
n_comp <- n.comp.nb(wm_d72)
n_comp$nc
```

The results confirms that there is an isolated island in the result,

we can plot the map of neighbours by using the `plot` function to determine if the generated map is of good use.

```{r}
plot(nga_wp$geometry, border="lightblue")
plot(wm_d72, coord, add=TRUE)
plot(nga_wp$geometry, border="lightblue")
plot(k1, coord, add=TRUE, col="red", length = 0.1)
```

-   The black lines show the links of neighbours within the cut-off distance of 72km.

-   The red lines show the links of 1st nearest neighbours

It looks like it is not wise to use fixed weight distance matrix as some LGAs has alot of neighbours within 72km, it could really skew the spatial correlation giving them more weights to these areas based on the resultant map. We will work with adaptive distance weight matrix instead.

#### Adaptive Weight distance matrix

By enforcing symmetry or accepting asymmetric neighbours, as shown in the code below, it is possible to control the number of neighbours of each region using the k nearest neighbour (knn) algorithm. In this case we set k to 8 as discussed in class. We use `str()` to display the result

```{r}
knn8 = knn2nb(knearneigh(coord, k=8))
str(knn8)
```

#### Plotting distance based neighbours

```{r}
plot(nga_wp$geometry, border="lightblue")
plot(knn8, coord, pch = 19, cex = 0.6, add = TRUE, col = "red")
knn8
```

This is a much better plot than the fixed distance matrix, as symmetry is enforced.

## Computing Spatial Autocorrelation

Now that we are ready we our weight matrix, we can begin to compute spatial autocorrelation
We will start with ***GLOBAL*** spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial correlation

### Computing Spatial Autocorrelation: Moran\'s I

#### Moran's I - The Null Hypothesis 

*The null hypothesis is to assume that non functional water points are randomly distributed between the different LGAs.*

#### Computing Moran's I

We will perform Moran\'s I statistical test with `moran.test()` of the **spdep** package. We will need to convert the knn8 weight matrix to a listw object first using `nb2listw()`

```{r}
knn8ListW = nb2listw(knn8)
moran.test(nga_wp$`wpt non functional`, listw = knn8ListW, zero.policy = TRUE, na.action = na.omit)
```

Based on the result, we will ***reject*** the null hypothesis as the p-value is less than 0.05. In fact as the p-value is less than 0.01, we can consider that as **highly significant.** The non functional water points are spatially clustered based on Moran\'s I statistics and are not random.

#### Computing Spatial Autocorrelation: Moran\'s I with Monte Carlo simulation

In order to confirm that the null hypothesis is false, we could use Monte Carlo simulation to predict potential outcomes of the event by using `moran.mc()` function of the **spdep** package. We will use 1000 simulations for this test and are not random.

```{r}
set.seed(1234)
global_moran_mc = moran.mc(nga_wp$`wpt non functional`, listw = knn8ListW, nsim=999, zero.policy = TRUE, na.action = na.omit)
global_moran_mc
```

Based on the simulation result, we will ***reject*** the null hypothesis as the p-value is less than 0.05. In fact as the p-value is 0.01, we can consider that as **highly significant.**

The non functional water points in the LGAs are not randomly distributed. Thus, since the Moran I statistics (0.38) is greater than 0, they are spatially clustered.

#### Visualizing Monte Carlo Moran\'s I

We will examine the simulated Moran\'s I test statistics in detail. This can be done by computing the mean, variance and standard deviation and summary statistics

```{r}
mean(global_moran_mc$res[1:999]) #compute mean
var(global_moran_mc$res[1:999]) #compute variance
sd(global_moran_mc$res[1:999]) #compute std dev.
summary(global_moran_mc$res[1:999])
```

We can plot the statistical values as a histogram using ggplot, however we need to convert the result into a data frame first

```{r}
df = data.frame(global_moran_mc$res) #convert to data frame

ggplot(df, aes(global_moran_mc$res)) + #aes = column name
  geom_histogram(bins=100, 
                 color="White", 
                 fill="lightblue") +
  labs(x = "Simulated Moran's I",
       y = "Frequency") + 
  xlim(-0.07, 0.07) + ylim(0, 50) +
  geom_vline(aes(xintercept=0),   
               color="red", linetype="dashed", size=1)
```

Based on the histogram, we can conclude that there is a positive correlation based on the result of the histogram for Moran\'s I Statistics, i.e. non functional water points in NGAs are clustered, and not disperse

### Computing Spatial Autocorrelation: Geary\'s C

#### Geary's C - The Null Hypothesis 

*The null hypothesis is to assume that non functional water points are clustering for either similar or dissimilar values are random.*

#### Computing Geary's C 

We will perform Geary\'s C statistical test with `geary.test()` of the **spdep** package.

```{r}
geary.test(nga_wp$`wpt non functional`, listw = knn8ListW)
```

Based on the result, we will ***reject*** the null hypothesis as the p-value is less than 0.05. In fact as the p-value is less than 0.01, we can consider that as **highly significant.**

Therefore, we can conclude that the non functional water points are clustering for either similar or dissimilar values, they are not randomly distributed based on Geary\'s C statistics

#### Computing Spatial Autocorrelation: Geary's C with Monte Carlo simulation

In order to further confirm that the null hypothesis is false, we could use Monte Carlo simulation to predict potential outcomes of the event by using geary`.mc()` function of the **spdep** package. We will use 1000 simulations for this test

```{r}
set.seed(1234)
global_geary_mc = geary.mc(nga_wp$`wpt non functional`, listw = knn8ListW, nsim=999)
global_geary_mc
```

Based on the simulation result, we will ***reject*** the null hypothesis as the p-value is less than 0.05. In fact as the p-value is 0.01, we can consider that as **highly significant.**

The non functional water points in the LGAs are clustering for either similar or dissimilar values, they are not randomly distributed. Thus, since the Geary's C statistics (0.61) is lesser than 1, they are spatially clustered.

#### Visualizing Monte Carlo Geary's C 

We will examine the simulated Geary's C test statistics in detail. This can be done by computing the mean, variance and standard deviation and summary statistics

```{r}
mean(global_geary_mc$res[1:999]) #compute mean
var(global_geary_mc$res[1:999]) #compute variance
sd(global_geary_mc$res[1:999]) #compute std dev.
summary(global_geary_mc$res[1:999])
```

We can plot the statistical values as a histogram using ggplot, however we need to convert the result into a data frame first

```{r}
df_G = data.frame(global_geary_mc$res) #convert to data frame

ggplot(df_G, aes(global_geary_mc$res)) + #aes = column name
  geom_histogram(bins=100, 
                 color="White", 
                 fill="lightblue") +
  labs(x = "Simulated Geary's C",
       y = "Frequency") +
  xlim(0.9, 1.1) + ylim(0, 50) +
  geom_vline(aes(xintercept=1),   
               color="red", linetype="dashed", size=1)
```

Based on the histogram, we can conclude that there is a positive correlation based on the result of the histogram for Geary\'s C Statistics, i.e. non functional water points in NGAs are clustered

It is interesting to note that in Moran I the smaller the number, indicates negative correlation (small -\> -ve), in contrast in Geary\'s C the smaller the number indicates positive correlation (small -\> +ve)

## Spatial Correlogram

## Reference

Kassambara A (n.d) . *K-Means Clustering in R: Algorithm and Practical Example*s

https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/

Long, A (n.d.), Local Moran

http://ceadserv1.nku.edu/longa//geomed/stats/localmoran/localmoran.html
