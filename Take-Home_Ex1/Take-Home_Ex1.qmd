---
title: "Take Home Exercise 1 - Investigation of Water points in Nigeria"
author: "Allan Chong"
editor: visual
execute: 
  warning: false
  message: false
---

## Overview

Water is a crucial resource for humanity. People must have access to clean water in order to be healthy. It promotes a healthy environment, peace and security, and a sustainable economy. However, more than 40% of the world's population lacks access to enough clean water. According to UN-Water, 1.8 billion people would live in places with a complete water shortage by 2025. One of the many areas that the water problem gravely threatens is food security. Agriculture uses over 70% of the freshwater that is present on Earth.

The severe water shortages and water quality issues are seen in underdeveloped countries. Up to 80% of infections in developing nations are attributed to inadequate water and sanitation infrastructure.

Despite technological advancement, providing rural people with clean water continues to be a key development concern in many countries around the world, especially in those on the continent of Africa.

The spatial patterns of non-functional water points will be shown in this study by using the proper global and local spatial association methodologies. We look at Nigeria's in this assignment.

## Getting Started

First, the required packages are loaded into the R environment . The required packages are **sf,** **tidyverse**, **spdep**, **tmap**, & **funModeling**

with the code below:

```{r}
pacman::p_load(sf, tidyverse, spdep, tmap, funModeling)
```

### Spatial Data

The spatial dataset used in this assignment is the Nigeria Level-2 Administrative Boundary spatial dataset downloaded from Center for Humanitarian Data - [Nigeria - Subnational Administrative Boundaries](https://data.humdata.org/dataset/cod-ab-nga)

We will load the spatial features by using `st_read()` from the **sf** package

As the data we want is in WSG-84 format, we set *crs* to 4326.

We won't utilize st transform() at this time because it can result in outputs with missing points after transformation, which would skew our study.

```{r}
nga = st_read(dsn = "data/geospatial",
               layer = "nga_admbnda_adm2_osgof_20190417",
               crs = 4326)

#nigeria_lga_sf = st_transform(nigeria_lga_sf, crs=4326) cause missing points

```

We could use `st_crs()`to verify the coordinate system from the object.

```{r}
st_crs(nga)
```

Before we start analyzing the data, lets us take a look at some characteristics of the spatial features to have a sense of what we are dealing with. We can use *`glimpse()`* to determine to accomplish that

```{r}
glimpse(nga)
```

We can use `freq()` of the **funModeling** package to display the distribution of Level 1 administration (Which are states in Nigeria) instead and only zooming in on the micro level when we perform the water point analysis.

```{r}
freq(data=nga, input = 'ADM1_EN')
```

774 Local Government Areas (LGA) make up Nigeria's 37 states, with Kano having the most LGAs overall.

For a meaningful analysis, there are just too many LGAs, both large and little.

Calling `ttm()` in the **tmap** package will switch the tmap's viewing mode to interactive viewing, which will help us better visualize the map. Without this change, the generated map will be too small for any type of analysis. Additionally, we'll base the map's plot on States (Level 1 Administration Area)

Given that there are 37 states, we must raise the maximum number of categories from the default value of 30 to 37. Using `tmap_options(max.categories = 37)`, the threshold can be set.

```{r}
ttm()
tmap_options(max.categories = 37)
```

Now, we are ready to build our map with the functions in the **tmap** package

```{r}

tm_shape(nga) + 
 
  tm_polygons("ADM1_EN") +
  tm_borders(alpha=0.5) + 
  tm_scale_bar() +
  tm_grid (alpha=0.2) +
  tm_layout(main.title="Map of Nigeria LGA", 
            main.title.position="center", 
            main.title.size=1.2, 
            legend.height = 0.35, 
            legend.width = 0.35, 
            frame = TRUE) 
```

### Aspatial Data

#### Cleaning the Data

The aspatial dataset used in this assignment is the water point data exchange dataset found in [WPdx Global Data Repositories](https://www.waterpointdata.org/access-data/). Data is filtered on the web portal to only keep Nigeria and the file is saved as *NigeriaWaterPoints_Raw.csv*

As we are only interested in the functionality of the water point, it is important to capture fields that may affect the functionality

-   LGA: The area we are interested in

-   State: The state of the LGA of Nigeria

-   Functional: Whether it is functional or not

-   management: who manages it?

-   Quality: what is the quality?

-   Water Source Category: where the water came from?

-   Water Tech Category: What technology is used?

-   latitude

-   longitude

To load the raw data file, we use the `read_csv` function

```{r}
#| eval: false
wpdx_raw = read_csv("data/aspatial/NigeriaWaterPoints_Raw.csv") 
```

Most of the columns are irrelevant, so we will perform the following:

-   keep the columns we want to clean it up by specifying the columns with one to retain with `subset`

-   renaming the columns using `rename_with`

-   Replace all the NA with unknown for columns with NA value present

```{r}
#| eval: false
retain_cols = c('#clean_adm2', '#clean_adm1', '#status_clean', '#management_clean', '#subjective_quality', '#fecal_coliform_presence', '#water_source_category', '#water_tech_category', '#lat_deg', '#lon_deg' )

new_col_names = c('LGA', 'State', 'Functional', 'Management', 'Quality', 'presence_of_fecal_coliform', 'Water_Source_Category', 'Water_Tech_Category', 'latitude', 'longitude')

wpdx_clean = subset(wpdx_raw, select = (names(wpdx_raw) %in% retain_cols)) %>%  rename_with(~ new_col_names, all_of(retain_cols)) %>% 
replace_na(list(Functional = "Unknown", Management = "Unknown", Quality = "Unknown", Water_Source_Category = "Unknown", Water_Tech_Category = "Unknown"))



```

We save the clean file with `saveRDS()`, the file will be reduced to 1.6MB from the 144MB raw file that we downloaded.

```{r}
#| eval: false
saveRDS(wpdx_clean, "data/aspatial/wpdx_clean.rds")
```

We can then delete the raw file from the project and retrieve the saved RDS file using `readRDS()`

```{r}
wpdx_clean = readRDS("data/aspatial/wpdx_clean.rds")

```

#### Converting csv data into spatial features

We can use `st_as_sf`to create a dataframe from the longitude (*x*) and latitude (*y*) values. The EPSG 4326 code is used as the dataset is referencing WGS84 geographic coordinate system. We could use `st_crs()`to verify the coordinate system from the object.

```{r}
wpdx_clean_sf = st_as_sf(wpdx_clean, coords = c("longitude", "latitude"), crs=4326)
st_crs(wpdx_clean_sf)
```

We can then use *`glimpse()`* to verify each field's data type & available values.

There are 95, 008 water points in the LGAs. The results also shows that the longitude and latitude values have been converted to a geometry object consisting of the longitude and latitude values as points, with both columns now dropped.

```{r}
glimpse(wpdx_clean_sf)
```

#### Aggregate the Data

We can use `freq()` of the **funModeling** package to display the distribution of *functional* field in *wpdx_clean_sf*. This is to help us aggregate the data as the dataset provide breakdowns of functional status. In order to only look at non functional water points, we will need to aggregate the different categories into simply functional, non functional and unknowns.

```{r}
freq(data=wpdx_clean_sf, input = 'Functional')
```

To aggregate them into functional, non functional and unknown, we will create new data frames to store them by using the `filter` function

```{r}
func_list = c("Functional", "Functional but needs repair", "Functional but not in use")
wpt_functional = wpdx_clean_sf %>%
  filter(Functional %in% func_list)

wpt_non_functional = wpdx_clean_sf %>%
  filter(!Functional %in% c(func_list, "Unknown"))

wpt_unknown = wpdx_clean_sf %>%
  filter(Functional %in% "Unknown")
```

Out of the 32, 204, records, we can gain some insights on why it might be non functional, is it due to management? Is it due to technology? Is it due to the source of the water?

Similarly, like how we aggregate functional data points, we could use `freq()` of the **funModeling** package to find out

```{r}
freq(data=wpt_non_functional, input = 'Management')
```

```{r}
freq(data=wpt_non_functional, input = 'Water_Tech_Category')
```

```{r}
freq(data=wpt_non_functional, input = 'Water_Source_Category')
```

From the results, we can conclude that

-   More than half of the non functional water points have an unknown management, we could ask if these water points are even managed.

-   Most of the non functional water points uses pumps, we could ask the question if there is an issue with the pumps and if there is a lack of expertise to repair or replace them when they fail.

-   97.72% of such non functional water points are made up of wells.

## Combining Spatial & Aspatial Data

We can use `st_intersects()` to find common data points between geographical datasets. In our case we need to find the common points in the Nigeria's LGA spatial dataset and the water point aspatial dataset

The below code does 4 things

1.  It intersects the Nigeria LGA dataset (*nga* dataframe) with the water point dataset (*wpdx_clean_sf* dataframe) and produce a new column to denote the total number of water points in the area (*Total wpt*) by using `mutate()` and `lengths()`

2.  Similar to step 1, the result of step 1 is piped to add 3 columns to denote the number of functional, non functional and unknown water points in the area to produce *wpt functional*, *wpt non functional* and *wpt unknown* respectively

3.  We also add 2 new columns to find the percentage of functional and non functional water points by using `mutate()`

4.  Select appropriate columns required which are the LGA area and LGA code (Column 3 & 4), Administration Level 1 Area and Administration Level 1 Code (Column 8 & 9) which represent states, the columns that were added as explained in steps 2 & 3 and the geometry multipolygon objects (Column 18 to 23) using `select()`

```{r}
nga_wp = nga %>% 
  #combine nga with water point sf
  mutate(`total wpt` = lengths(
    st_intersects(nga, wpdx_clean_sf))) %>%
  #add columns to produce no. of functional, non functional and unknown points
  mutate(`wpt functional` = lengths(
    st_intersects(nga, wpt_functional))) %>%
  mutate(`wpt non functional` = lengths(
    st_intersects(nga, wpt_non_functional))) %>%
  mutate(`wpt unknown` = lengths(
    st_intersects(nga, wpt_unknown))) %>%
  
  #add columns to compute %
  mutate(pct_functional = `wpt functional`/`total wpt`) %>%
  mutate(`pct_non-functional` = `wpt non functional`/`total wpt`) %>%
  select(3:4, 8:9, 18:23)
```

## Visualizing the spatial distribution of water points

We could use breaks of the summary statistics by using percentiles, this is to help us find out the distribution of water points in each quantile.

```{r}
#summary(nga_wp$`total wpt`)
#summary(nga_wp$`wpt functional`)
summary(nga_wp$`wpt non functional`)
#summary(nga_wp$`wpt unknown`)
```

It is recommended not to use the default style with breaks as quantile since the range from the third quantile to the maximum is too wide and could result in a skewed representation. We compute the variance and standard deviation of non-functional water points first to better understand our dataset since we now need to decide which style is appropriate for the map.

```{r}
var(nga_wp$`wpt non functional`)
sd(nga_wp$`wpt non functional`) 
```

It appears that this dataset has a very large variance.  Since the variance is so high, we would like to lower it. Using the *kmeans* style is one method to do this. *n = 6* is choosen as after some experimentation, it appears that 6 is the optimal number of clusters.

Functions from the **tmap** packages is used to produce the map

First we use `tm_shape()` `+ tm_fill("ADM1_EN")` to form Layer 1 of the map to form the 37 states of the map. The *Pastel1* palette is used because it is difficult to read different shades of the same two to three colors; Pastel1 has more colors, making states more distinct.

Next we use `tm_shape()` `+ tm_fill("wpt non functional")` to form Layer 2 of the map which are the non functional water points. The palette used in this case is Purple Red such that areas with very little water points are shaded with a very light colour.

We may switch between layers on the interactive map to superimpose the nonfunctional water locations. With so many polygons, putting it side by side can be challenging to interpret.

```{r}
tm_shape(nga) + 
 
  tm_fill("ADM1_EN", palette = "Pastel1") +
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +

  tm_shape(nga_wp) + 
  tm_fill("wpt non functional", 
          palette ="PuRd", style="kmeans", n=6) +  
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +
  tm_layout(main.title="non functional WP - 2 Layer map", 
            main.title.position="center", 
            main.title.size=1.2, 
            #legend.height = 0.35, 
            #legend.width = 0.35, 
            frame = TRUE) 


```

Using **dplyr** package, we can summarize find out which States has the most number non functional water points and which are the states that has the most number of LGAs by using the functions `group_by`, `summarise` and `arrange`

```{r}
#Sum of non functional water points 
nga_wp %>% 
  group_by(ADM1_EN) %>% 
   summarise(NF_Frequency = sum(`wpt non functional`), 
             #F_Frequency = sum(`wpt functional`),
             Total_Freq = sum(`total wpt`),
             NF_Ratio = (NF_Frequency / Total_Freq) * 100
             ) %>% 
    arrange(desc(NF_Frequency))

#sum of LGAs by states 
nga_wp %>% 
  group_by(ADM1_EN) %>% 
  summarise(count = n())%>% 
    arrange(desc(count))
```

### Observations

-   According to the statistics, Osun has the highest number of non-operational water points - 2118 of them among the 37 states, followed by Kaduna (1912 water points) and Kwara (1634 water points).

-   Kano, despite being the State with the most number of LGAs (44), has only 1570 non functional water points (Ranked 4th) as compared to Osun that only comprises of 30 LGAs (Ranked 1st).

-   In contrast to Kaduna & Kwara, which are greater in size, Osun has 5519 water points, which is an interesting fact. In addition, nearly half of the water points in Kwara are not working.

-   Despite having a larger territory, Ondo, the state directly south-east of Osun, has over 60% of its water points that are not operational.

-   The south-eastern and western regions of Nigeria appear to be the hotspots for the spread of inoperative water points.

-   There are no non-functional water points on Nigeria's north-eastern coast. Using the tmap package, we plot the functional map to see if there are any water points in the region or if there are none at all.

    This can assist us in figuring out whether the region in the north-east is succeeding in a way that can be transferred to other parts of the nation, or whether it is uninhabited or underdeveloped.

```{r}
  tm_shape(nga_wp) + 
  tm_fill("wpt functional", 
          palette ="PuRd", style="kmeans", n=6) +  
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +
  tm_layout(main.title="non functional WP - 2 Layer map", 
            main.title.position="center", 
            main.title.size=1.2, 
            #legend.height = 0.35, 
            #legend.width = 0.35, 
            frame = TRUE) 
```

The north-eastern region of Nigeria has few to no water points, which suggests to us that it may be that the region is underdeveloped or uninhabited.

## Spatially Constrained Cluster Analysis

### Computing Spatial Weights

We need to find the spatial weights first before we can compute global spatial correlation statistics. The spatial weights is used to define the neighbourhood relationships between the geographical units

We use `poly2nb()` of **spdep** package to compute the contiguity weight matrix. The function builds a neighbour list based on regions with contiguous boundaries. Using queen's contiguity weight matrix, we have

```{r}
wm_q = poly2nb(nga_wp)
summary(wm_q)
```

From the results, there are 774 regions in Nigeria,

Using the Queen's method, 1 of them has 14 neighbours, 1 of them is an isolated island (no neighbour), and 2 of them only has 1 neighbour

### Building the weights matrix

After computing the spatial weights, we will need to build the weights matrix. From the result, of the neighbour list, there exist an isolated island. With this island, it is not recommended to use contiguity weight matrix as this island will have no neighbours at all.

With that in mind, we will use distance based weight matrix instead.

By using `dnearneigh()` of **spdep** package, we can determine the distance based weight matrix. The function looks for neighbours of regions points by Euclidean distance between the *lower (\>=) and upper (\<=) bound* or with the parameter `longlat = True` by great circle distance in km

### Building the distance based weight matrix

#### Fixed Weight distance matrix

The steps involved:

1.  Using the k nearest neighbour (knn) algorithm, we can return a matrix with indices of points that belongs to the set of k nearest neighbours of each others by using `knearneigh()` of **spdep**

2.  Convert the knn objects into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using `knn2nb()`

3.  Return the length of neighbour relationship edges by using `nbdists()` of **spdep**. The function returns in the units of coordinates if the coordinates are projected, in km otherwise.

4.  Remove the list structure of the return objects by using `unlist()`

Before we can start building the matrix, we first need to find the coordinates

-   The longitude is the first variable in each centroid, this enables us to obtain only the longitude.

-   The latitude is the second variable in each centroid, this enables us to obtain only the latitude

Using the double bracket notation \[\[\]\] and the index, we can access the latitude & longitude values.

```{r}
longitude = map_dbl(nga_wp$geometry, ~st_centroid(.x)[[1]]) #longitude index 1
latitude = map_dbl(nga_wp$geometry, ~st_centroid(.x)[[2]]) #latitude index 2
```

After getting the longitude and latitudes, we can form the coordinates object named `coord` using `cbind`.Using the `head` function, we can inspect the elements of `coord` to verify if they are correctly formatted

```{r}
coord = cbind(longitude, latitude)
head(coord)
```

#### Find the lower and upper bounds

```{r}
#returns a list of nb objects from the result of k nearest neighbours matrix, Step 1 & 2
k1 = knn2nb(knearneigh(coord)) 
#return the length of neighbour relationship edges and remove the list structures, Step 3 & 4
k1dist = unlist(nbdists(k1, coord, longlat = TRUE)) 
summary(k1dist)
```

From the result, the largest first nearest neighbour is 71.66 km, hence by using this as the upper bound, we can be certain that all units will have at least 1 neighbour

#### Creating the fixed distanced weight matrix

`dnearneigh` will be used to compute the distance weight matrix

```{r}
wm_d72 = dnearneigh(coord, 0, 72, longlat = TRUE)
wm_d72
```

The average number of links denotes the number of non zero links divided by the number of regions. In this case, a region has about on average between 24 neighbours

We will display the structure of the weight matrix is to combine `table()` and `card()` of spdep.

-   The `card()` function counts the neighboring regions in the neighbours list.

-   `table()` creates a contingency table of the counts for each combination of factor levels using cross-classifying factors.

```{r}
table(nga_wp$ADM2_EN, card(wm_d72))
```

To get the number of disconnected connected subgraphs in the *wmd72* weight matrix, we can use the `n.comp.nb()` function of the **spdep** package.

```{r}
n_comp = n.comp.nb(wm_d72)
n_comp$nc
```

The results confirms that there is an isolated island in the result. and hence we will not used fixed weight matrix.It looks like it is not wise to use fixed weight distance matrix as some LGAs has alot of neighbours within 72km, it could really skew the spatial correlation giving them more weights to these areas based on the resultant map. We will work with adaptive distance weight matrix instead.

#### Adaptive Weight distance matrix

By enforcing symmetry or accepting asymmetric neighbours, as shown in the code below, it is possible to control the number of neighbours of each region using the k nearest neighbour (knn) algorithm. In this case we set k to 8 as discussed in class. This can be accomplished by calling `knn2nb()` and `knearneigh()`. We will use `str()` to display the result

```{r}
knn8 = knn2nb(knearneigh(coord, k=8))
str(knn8)
```

#### Plotting distance based neighbours

```{r}
plot(nga_wp$geometry, border="lightblue")
plot(knn8, coord, pch = 19, cex = 0.6, add = TRUE, col = "red")
knn8
```

We did not adjust the projection from WSG84 in the earlier part because we needed to perform `st_transform()` which requires the data to be in WGS84 format.

Since the spatial feature data frame has been fully constructed, we can now use `st_transform()` to update the appropriate projection system. The Nigeria Mid Belt Coordinate System (26392) will be applied, and `st crs()` is used to confirm the transformation was completed.

```{r}
nga = st_transform(nga, crs=26392)
st_crs(nga)
nga_wp = st_transform(nga_wp, crs=26392)
st_crs(nga_wp)
```

## Computing Spatial Autocorrelation

Now that we are ready with our weight matrix, we can begin to compute spatial autocorrelation We will start with ***GLOBAL*** spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial correlation

### Computing Spatial Autocorrelation: Moran's I

#### Moran's I - The Null Hypothesis

*The null hypothesis is to assume that non functional water points are randomly distributed between the different LGAs.*

#### Computing Moran's I

We will perform Moran's I statistical test with `moran.test()` of the **spdep** package. We will need to convert the knn8 weight matrix to a listw object first using `nb2listw()`

```{r}
knn8ListW = nb2listw(knn8)
moran.test(nga_wp$`wpt non functional`, listw = knn8ListW, zero.policy = TRUE, na.action = na.omit)
```

Based on the result, we will ***reject*** the null hypothesis as the p-value is less than 0.05. In fact as the p-value is less than 0.01, we can consider that as **highly significant.** The non functional water points are spatially clustered based on Moran's I statistics and are not random.

#### Computing Spatial Autocorrelation: Moran's I with Monte Carlo simulation

In order to confirm that the null hypothesis is false, we could use Monte Carlo simulation to predict potential outcomes of the event by using `moran.mc()` function of the **spdep** package. We will use 1000 simulations for this test and are not random.

```{r}
set.seed(1234)
global_moran_mc = moran.mc(nga_wp$`wpt non functional`, listw = knn8ListW, nsim=999, zero.policy = TRUE, na.action = na.omit)
global_moran_mc
```

Based on the simulation result, we will ***reject*** the null hypothesis as the p-value is less than 0.05. In fact as the p-value is 0.01, we can consider that as **highly significant.**

The non functional water points in the LGAs are not randomly distributed. Thus, since the Moran I statistics (0.38) is greater than 0, they are spatially clustered.

#### Visualizing Monte Carlo Moran's I

We will examine the simulated Moran's I test statistics in detail. This can be done by computing the mean, variance and standard deviation and summary statistics

```{r}
mean(global_moran_mc$res[1:999]) #compute mean
var(global_moran_mc$res[1:999]) #compute variance
sd(global_moran_mc$res[1:999]) #compute std dev.
summary(global_moran_mc$res[1:999])
```

We can plot the statistical values as a histogram using ggplot, however we need to convert the result into a data frame first

```{r}
df = data.frame(global_moran_mc$res) #convert to data frame

ggplot(df, aes(global_moran_mc$res)) + #aes = column name
  geom_histogram(bins=100, 
                 color="White", 
                 fill="lightblue") +
  labs(x = "Simulated Moran's I",
       y = "Frequency") + 
  xlim(-0.07, 0.07) + ylim(0, 50) +
  geom_vline(aes(xintercept=0),   
               color="red", linetype="dashed", size=1)
```

Based on the histogram, we can conclude that there is a positive correlation based on the result of the histogram for Moran's I Statistics, i.e. non functional water points in NGAs are clustered, and not disperse

### Computing Spatial Autocorrelation: Geary's C

#### Geary's C - The Null Hypothesis

*The null hypothesis is to assume that non functional water points are clustering for either similar or dissimilar values are random.*

#### Computing Geary's C

We will perform Geary's C statistical test with `geary.test()` of the **spdep** package.

```{r}
geary.test(nga_wp$`wpt non functional`, listw = knn8ListW)
```

Based on the result, we will ***reject*** the null hypothesis as the p-value is less than 0.05. In fact as the p-value is less than 0.01, we can consider that as **highly significant.**

Therefore, we can conclude that the non functional water points are clustering for either similar or dissimilar values, they are not randomly distributed based on Geary's C statistics

#### Computing Spatial Autocorrelation: Geary's C with Monte Carlo simulation

In order to further confirm that the null hypothesis is false, we could use Monte Carlo simulation to predict potential outcomes of the event by using geary`.mc()` function of the **spdep** package. We will use 1000 simulations for this test

```{r}
set.seed(1234)
global_geary_mc = geary.mc(nga_wp$`wpt non functional`, listw = knn8ListW, nsim=999)
global_geary_mc
```

Based on the simulation result, we will ***reject*** the null hypothesis as the p-value is less than 0.05. In fact as the p-value is 0.01, we can consider that as **highly significant.**

The non functional water points in the LGAs are clustering for either similar or dissimilar values, they are not randomly distributed. Thus, since the Geary's C statistics (0.61) is lesser than 1, they are spatially clustered.

#### Visualizing Monte Carlo Geary's C

We will examine the simulated Geary's C test statistics in detail. This can be done by computing the mean, variance and standard deviation and summary statistics

```{r}
mean(global_geary_mc$res[1:999]) #compute mean
var(global_geary_mc$res[1:999]) #compute variance
sd(global_geary_mc$res[1:999]) #compute std dev.
summary(global_geary_mc$res[1:999])
```

We can plot the statistical values as a histogram using ggplot, however we need to convert the result into a data frame first

```{r}
df_G = data.frame(global_geary_mc$res) #convert to data frame

ggplot(df_G, aes(global_geary_mc$res)) + #aes = column name
  geom_histogram(bins=100, 
                 color="White", 
                 fill="lightblue") +
  labs(x = "Simulated Geary's C",
       y = "Frequency") +
  xlim(0.9, 1.1) + ylim(0, 50) +
  geom_vline(aes(xintercept=1),   
               color="red", linetype="dashed", size=1)
```

Based on the histogram, we can conclude that there is a positive correlation based on the result of the histogram for Geary's C Statistics, i.e. non functional water points in NGAs are clustered

It is interesting to note that in Moran I the smaller the number, indicates negative correlation (small -\> -ve), in contrast in Geary's C the smaller the number indicates positive correlation (small -\> +ve)

## Spatial Correlogram

Spatial correlograms make it easy to examine spatial autocorrelation patterns in the data.

They are graphs of some measure of autocorrelation (Moran's I or Geary's c) against distance and they demonstrate how correlated pairs of spatial observations are as one increase the distance (lag) between them.

### Computing Moran's I correlogram

We use `sp.correlogram()` of **spdep** package to compute a 8-lag spatial correlogram of non functional water points. The global spatial autocorrelation used in Moran's I

We use `zero.policy = TRUE` because there is an isolated island, if we don't use it, an error will be thrown to say there is no neighbours. We use order of 8 as we are using adaptive weight matrix of 8 neighbours. We also use binary weights for the style as it is more robust.

```{r}
MI_Corr = sp.correlogram(wm_q, nga_wp$`wpt non functional`, order = 8, method = "I", style = "B", zero.policy = TRUE)
plot(MI_Corr)
```

Plotting the output might not allow us to provide complete interpretation, this is because not all autocorrelation values are statistically significant. Hence we should analyze the report by printing out the result

```{r}
print(MI_Corr)
```

-   The p value is \< 0.01 and hence is highly statically significant for all values except the 7th neighbour.

-   We can conclude that non functional water points is positively correlated for LGAs up to a distance of 6 neighbours, and negatively correlated from the 8th neighbour onwards.

-   As the 7th degree neighbour is not statistically significant, we will **not reject** the null hypothesis of it being random.

### Computing Geary's C correlogram

Similarly to Moran's I correlogram, we use `sp.correlogram()` of **spdep** package to compute a 8-lag spatial correlogram of non functional water points

We use `zero.policy = TRUE` because there is an isolated island, if we don't use it, an error will be thrown to say there is no neighbours. We use order of 8 as we are using adaptive weight matrix of 8 neighbours. We also use binary weights for the style as it is more robust.

```{r}
GC_Corr = sp.correlogram(wm_q, nga_wp$`wpt non functional`, order = 8, method = "C", style = "B", zero.policy = TRUE)
plot(GC_Corr)
```

Plotting the output might not allow us to provide complete interpretation, this is because not all autocorrelation values are statistically significant. Hence we should analyze the report by printing out the result

```{r}
print(GC_Corr)
```

-   In this case, it is only statistically significant for the 1st to 4th degree and 8th degree neighbour for non functional water points to be correlated by distance. The rest of the neighbours are insignificant and appears to be random for the Geary's C statistics.

-   In order to build higher order neighbour lists, we can use the `nblag()` function, where higher order neighbours are only lags links apart from one another on the graph defined by the input neighbours list. (Treglia, 2015).

    To do so, we can iterate through the neighbour lists (*nb8*) with `sapply()` and find the distance between the point of lag (y) and the coordinate using `nbdists()`, unlist them using `unlist()`, and find the mean using `mean()`.

```{r}
#find spatial lag up to 8 neighbours
nb8 <- nblag(wm_q, 8)
#sapply to iterate, passing each value as y
#Find the distance between y and the coordinates
#unlist them and find the mean
correlogram_dist = sapply(nb8, function(y) mean(unlist(nbdists(y, coord, longlat = TRUE))))
correlogram_dist
```

From spatial lag 1 to 4 (from 36.62 km - 155.79 km), the non-functional water points are clustered, and from lag 8 onwards, they are dispersed, beginning at 356.66 km.

## Cluster and Outlier Analysis

Statistics called Local Indicators of Spatial Association, or LISA, assess whether clusters exist in the spatial arrangement of a given variable.

We will use relevant ***Local Indicators for Spatial Association (LISA)***, particularly ***local Moran,*** in this section to identify clusters and/or outliers in the number of non functional water points in Nigeria.

### Computing Local Moran's I Statistics

The `localmoran()` function of **spdep** will be used to calculate local Moran's I. Given a collection of *l_i* values, *z_i* values and a *listw* object with neighbour weighting details for the polygon associated with the *z_i* values.

```{r}
fips = order(nga_wp$`wpt non functional`)
localMI = localmoran(nga_wp$`wpt non functional`, knn8ListW)
head(localMI)


```

Before mapping the local Moran\'s I map, we need to append the local Moran\'s I data frame (`localMI`) onto the nga_wp spatial polygon data frame by using `cbind()` and renaming the columns by Prefixing Pr.

```{r}
nga_wp.localMI = cbind(nga_wp, localMI) %>% #pipe
                rename(Pr.Ii = Pr.z....E.Ii..)

nga_wp.localMI_pValue = nga_wp.localMI
```

After creating the the new data frame `nga_wp.localMI`, we can use the **tmap** package to plot the local Moran\'s I values with a multi layer map

1.  We use `tm_shape()` `+ tm_fill("ADM1_EN")` to form Layer 1 of the map to form the 37 states of the map. The *Pastel1* palette is used because it is difficult to read different shades of the same two to three colors; Pastel1 has more colors, making states more distinct.

2.  We use `tm_shape()` `+ tm_fill("wpt non functional")` to form layer 2 of the map to plot the number of non functional water point.

3.  Next we use `tm_shape()` `+ tm_shape(nga_wp.localMI)` to form Layer 3 of the map which are the local Moran values. The palette used in this case is Purple Red such that areas with very little water points are shaded with a very light colour, in order to be consistent with our analysis, we use *kmeans* style and *n=6*

    As we expect that the choropleth map shows that there is evidence for both positive & negative li values, we need to consider the p-values for each of these values to determine if they are statistically significant

4.   We use `tm_shape()` `+ tm_shape(nga_wp.localMI_pValue)`to form Layer 4 of the map which are the local Moran p values. The palette used in this case is Green with fixed breaks denoting the various p value of confidence interval. Since we are only interested in values that are statistically significant, we stop at 0.05

We may switch between layers on the interactive map to superimpose the nonfunctional water locations. With so many polygons, putting it side by side can be challenging to interpret. We use the Green palette so that it can be contrasting when we toggle between maps

```{r}
tm_shape(nga) + 
  tm_fill("ADM1_EN", palette = "Pastel1") +
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +
  
tm_shape(nga_wp) + 
  tm_fill("wpt non functional", 
          palette ="Blues", style="kmeans", n=6) +  
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +


tm_shape(nga_wp.localMI) + 
  tm_fill(col="Ii", #note that actual value is Ii
          palette = "PuRd", title = "Local Moran's I Statistics",
          style="kmeans", n=6) + 
  tm_borders(alpha = 0.5) +
  
tm_shape(nga_wp.localMI_pValue) + 
  tm_fill(col="Pr.Ii", #note that p value is Pr.Ii
          breaks=c(-Inf, 0.001, 0.01, 0.05,  Inf),
          style="fixed",
          palette = "-Greens", title = "Local Moran's I p values") +   tm_borders(alpha = 0.5)

```

#### Analysis of Results of Local Moran\'s I Statistics - Dissimilar Features (\< 0)

blah blah blah

#### 
Analysis of Results of Local Moran\'s I Statistics - Similar Features (\>= 0)

blah blah blah

### Plotting Moran Scatterplot

-   A helpful visual tool for exploratory analysis is the Moran scatter plot, which helps one to judge how similar an observed value is to its nearby observations.

-   The y axis, also referred to as the response axis , is dependent on the values of the observations.

-   Based on the weighted average or spatial lag of the corresponding observation on the X axis, the Y axis is constructed.

```{r}
nci = moran.plot(nga_wp$`wpt non functional`, knn8ListW, 
                 labels=as.character(nga_wp$ADM2_EN),
                 xlab = "Non functional water points",
                 ylab="Spatially lag Non functional water points"
)
```

### Preparing LISA map classes

1.  Create the quadrants

    ```{r}
    quadrant = vector(mode="numeric",length=nrow(localMI))
    ```

2.  Compute the spatially lagged variable of interest (i.e. Non functional water points) and centers the spatially lagged variable around its mean.

    ```{r}
    nga_wp$`lag_wpt non functional` = lag.listw(knn8ListW, nga_wp$`wpt non functional`)
    DV = nga_wp$`lag_wpt non functional` - mean(nga_wp$`lag_wpt non functional`) 
    ```

3.  Center the local Moran\'s I value around the mean

    ```{r}
    #local moran
    LM_I <- localMI[,1] - mean(localMI[,1])    
    ```

4.  Setup the statistically significant levels for the local Moran

    ```{r}
    signif = 0.05
    ```

5.  Define the quadrants levels

    ```{r}
    #C_MI = Local Moran I
    quadrant[DV < 0 & LM_I > 0] = 1 #C_MI > 0 -> cluster, DV < 0 low-low pattern
    quadrant[DV < 0 & LM_I < 0] = 2 #C_MI < 0 -> outlier,  DV < 0 Low High
    quadrant[DV > 0 & LM_I < 0] = 3 #C_MI < 0 -> outlier,  DV < 0 High Low
    quadrant[DV > 0 & LM_I > 0] = 4 #C_MI > 0 -> cluster, DV > 0 high-high pattern
    ```

6.  Place non significant Moran into category 0

    ```{r}
    quadrant[localMI[,5]>signif] = 0
    ```

7.  Plotting the LISA Map

    ```{r}
    nga_wp.localMI$quadrant = quadrant
    colors = c("white", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
    clusters = c("insignificant", "low-low", "low-high", "high-low", "high-high")


    tm_shape(nga) + 
      tm_fill("ADM1_EN", palette = "Pastel1") +
      tm_borders(alpha=0.5) + 
      tm_grid (alpha=0.2) +
      
    tm_shape(nga_wp) + 
      tm_fill("wpt non functional", 
              palette ="Blues", style="kmeans", n=6) +  
      tm_borders(alpha=0.5) + 
      tm_grid (alpha=0.2) +
        
    tm_shape(nga_wp.localMI) + 
      tm_fill(col="quadrant", style="cat", 
              palette = colors[c(sort(unique(quadrant)))+1], 
              labels = clusters[c(sort(unique(quadrant)))+1]) +
              
              tm_borders(alpha=0.5) +
      
    tm_shape(nga_wp.localMI_pValue) + 
      tm_fill(col="Pr.Ii", #note that p value is Pr.Ii
              breaks=c(-Inf, 0.001, 0.01, 0.05,  Inf),
              style="fixed",
              palette = "-Greens", title = "Local Moran's I p values") +  
      tm_borders(alpha = 0.5)
    ```

## Hot Spot and Cold Spot Area Analysis

A region or value that is higher than its surrounds is referred to as a "hot spot", while value lower than its surrounding are known as cold spot in a variety of fields. Moran's I and Geary's C cannot distinguish them as they only indicate clustering, it is not possible to tell whether a cluster is a hot spot, cold spot or both.

### Getis and Ord's G-Statistics

The G statistic distinguishes between hot spots and cold spots. It identifies **spatial concentrations.**

> *It must be noted that the G Statistics is not able to detect spatial outliers as it does not take cross products like local Moran.*

The General G statistic is interpreted relative to its mean value. The value for which there is no spatial association

-   G \> mean implies potential clusters of "hot spots"

-   G \< mean implies potential clusters of "cold spots"

The analysis consists of three steps:

1.  Deriving spatial weight matrix, which we already have done, *wm_d72* is the fixed distance matrix and *knn8* is the adaptive weight matrix

2.  Computing Gi statistics

3.  Mapping Gi statistics

### Gi statistics using adaptive distance

We can use `localG()` to compute the Gi statistics. The output of `localG()` is a vector of G or Gstar values, with attributes \"`gstari`\" set to TRUE or FALSE, \"call\" set to the function call, and class \"localG\".

As we previously discussed, due to the fact there is an isolated island, we will not used fixed weights

```{r}
fips = order(nga_wp$`wpt non functional`)
gi.adaptive = localG(nga_wp$`wpt non functional`, knn8ListW, return_internals = TRUE)
gi.adaptive
```

The Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.

Next, we will join the Gi values to their corresponding nga_wp sf data frame by converting the output vector (i.e. `gi.fixed`) into r matrix object by using `as.matrix()` and join the nga_wp data frame into with the produced matrix with `cbind()` Lastly, the field name of the gi values is renamed to *gstat_fixed* by using `rename()`

```{r}
nga_wp.gi = cbind(nga_wp, as.matrix(gi.adaptive)) %>% #pipe
          rename(gstat_adaptive = as.matrix.gi.adaptive.)
```

Using the same way we plot multi layer map we previously discussed, we plot nigeria map at layer 1 as Nigeria Level 1 administration area, layer 2 as non functional water points, layer 3 as the G statistics and layer 4 as the local Moran I statistics to make our analysis with **tmap** functions below

```{r}
tm_shape(nga) + 
  tm_fill("ADM1_EN", palette = "Pastel1") +
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +
  
tm_shape(nga_wp) + 
  tm_fill("wpt non functional", 
          palette ="Greens", style="kmeans", n=6) +  
  tm_borders(alpha=0.5) + 
  tm_grid (alpha=0.2) +

tm_shape(nga_wp.gi) +
          tm_fill(col = "gstat_adaptive", 
          style = "pretty",
          palette="-RdBu",
          title = "local Gi") +
  tm_borders(alpha = 0.5) +
  
tm_shape(nga_wp.localMI) + 
  tm_fill(col="quadrant", style="cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1]) +
          
          tm_borders(alpha=0.5) 
```

#### Analysis of G Statistics 

blah blah blah

#### Comparing Local Moran I to G Statistics 

blah blah blah

## Reference

Harris N, Goldman E, Gabris C, Nordling J, Minnemeyer S, Ansari S, Lippmann M, Bennett L, Raad M, Hansen M, Potapov P (2017), *Using spatial statistics to identify emerging hot spots of forest loss*, T2 - Environmental Research Letters

https://dx.doi.org/10.1088/1748-9326/aa5a2f

Kassambara A (n.d) . *K-Means Clustering in R: Algorithm and Practical Example*s

https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/

Long, A (n.d.), Local Moran

http://ceadserv1.nku.edu/longa//geomed/stats/localmoran/localmoran.html

Treglia M (2015), *A simple correlogram using sp.correlogram with data(meuse)*

https://rpubs.com/erikaaldisa/spatialweights

R Coder (2022) *sapply function in R*

https://r-coder.com/sapply-function-r/
