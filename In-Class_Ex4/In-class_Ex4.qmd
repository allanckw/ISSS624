---
title: "In class exercise 4 - Hedonic Pricing Models using GWR and Visualizing coefficient estimates of 2015 Condo Resale price in Singapore"
author: "Allan Chong"
editor: visual
execute: 
  warning: false
  message: false
---

## Overview

Geographically weighted regression (GWR) is a spatial statistical technique that models the local relationships between these independent variables and an outcome of interest while taking non-stationary variables (such as climate, demographic factors, and physical environment characteristics) into account (also known as dependent variable).

We will practice using GWR techniques to construct Hedonic pricing models in this practical exercise.

The 2015 condominium resale prices is the dependent variable. There are two categories of independent variables: structural and locational.

Two data sets will be used in this model building exercise, they are:

-   URA Master Plan subzone boundary in shapefile format (i.e. *MP14_SUBZONE_WEB_PL*)

-   condo_resale_2015 in csv format (i.e. *condo_resale_2015.csv*)

## Getting Started

Before we begin, we need to install the necessary R packages into R and launch them in the R environment

The R packages requirements for this exercise are:

-   R package for building OLS and performing diagnostics tests

    -   [**olsrr**](https://olsrr.rsquaredacademy.com/)

-   R package for calibrating geographical weighted family of models

    -   [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/)

-   R package for multivariate data visualisation and analysis

    -   [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)

-   Spatial data handling

    -   **sf**

-   Attribute data handling

    -   **tidyverse**, especially **readr**, **ggplot2** and **dplyr**

-   Choropleth mapping

    -   **tmap**

We load the required packages in R with the code below.

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```

## What is in the GWModel Package

The [**GWmodel**](https://www.jstatsoft.org/article/view/v063i17) package offers a variety of localized spatial statistical methods, including GW summary statistics, GW principle components analysis, GW discriminant analysis, and other types of GW regression, some of which are offered in simple and reliable (outlier resistant) forms.

A handy exploratory tool that frequently comes before (and directs) a more conventional or complex statistical analysis is provided by mapping the outputs or parameters of the GWmodel.

## Spatial Data

### Loading the Master Plan 2014 Subzone Boundary

We will load the spatial data using st_read()

```{r}
mpsz = st_read(dsn="data/geospatial", layer="MP14_SUBZONE_WEB_PL")
```

By using `st_crs()` to inspect the sf object, the dataframe is still in WGS84 format. As such we will need to transform it to the SVY21 projection system by calling `st_transform()`

```{r}
st_crs(mpsz)
```

```{r}
mpsz_svy21 = st_transform(mpsz, 3414)
st_crs(mpsz_svy21)
```

From inspection, we know that *mpsz_svy21* is now referencing the correct projection system.

## Aspatial Data

To load the raw *condo_resale_2015*.csv data file, we use the `read_csv` function to import *condo_resale_2015* into R as a tibble data frame named *condo_resale*.

```{r}
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")

```

It's necessary that we check to see if the data file was successfully imported into R after importing it. We can use the `glimpse()` function to achieve that

```{r}
glimpse(condo_resale)
```

### Creating a feature dataframe from an Aspatial data frame with `st_as_sf()`

We can use `st_as_sf`to create a data frame from the *LONGITUDE* (*x*) and *LATITUDE* (*y*) values. The EPSG 4326 code is used as the dataset is referencing WGS84 geographic coordinate system.

We will need to call `st_transform()` such that it will be the same projection system as the mpsz_svy21 spatial feature data frame

```{r}
condo_resale_sf = st_as_sf(condo_resale, coords = c("LONGITUDE", "LATITUDE"), 
                           crs=4326) %>%
                  st_transform(crs=3414)
glimpse(condo_resale_sf)
```

We can use glimpse to take a look at the data, we can observe that the *LONGITUDE* and *LATITUDE* is dropped while the geometry column is created

## Exploratory Data Analysis (EDA)

### EDA using statistical graphics

We will use statistical graphics functions of **ggplot2** package to perform EDA.

We can plot the distribution of SELLING_PRICE of condo resales by using a histogram

```{r}
ggplot(data=condo_resale_sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light green")
```

We can observe that the selling price of condo resale apartments shows a right skewed distribution, which can help us conclude that the units were transacted at relatively lower price.

Statistically, we could use Log transformation to normalize the log distribution. It can be achieved by using `mutate()` of **dplyr** package.

```{r}
condo_resale_sf = condo_resale_sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

We then plot the histogram of the *LOG_SELLING_PRICE* as per the code below, the result of the transformation has resulted in a less skewed distribution.

```{r}
ggplot(data=condo_resale_sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light green")
```

### Multiple Histogram Plots distribution of variables

We will learn how to use the ggpubr package's ggarrange() function to create multiple histogram (also known as a trellis plot) in this part. The 12 histograms are generated using the code snippet below.

```{r}
AREA_SQM = ggplot(data=condo_resale_sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light green")

AGE = ggplot(data=condo_resale_sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light green")

PROX_CBD = ggplot(data=condo_resale_sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light green")

PROX_CHILDCARE = ggplot(data=condo_resale_sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light green")

PROX_ELDERLYCARE = ggplot(data=condo_resale_sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light green")

PROX_URA_GROWTH_AREA = ggplot(data=condo_resale_sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light green")

PROX_HAWKER_MARKET = ggplot(data=condo_resale_sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light green")

PROX_KINDERGARTEN = ggplot(data=condo_resale_sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light green")

PROX_MRT = ggplot(data=condo_resale_sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light green")

PROX_PARK = ggplot(data=condo_resale_sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light green")

PROX_PRIMARY_SCH = ggplot(data=condo_resale_sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light green")

PROX_TOP_PRIMARY_SCH = ggplot(data=condo_resale_sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light green")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

### Drawing Statistical Point Map

Lastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using **tmap** package.

We will turn on the interactive mode of tmap by using the `ttm().`

There may be issues with the map after reprojection, so we will set `tmap_options(check.and.fix = TRUE)`

The number of Plan Area N is 55, however the default number of categories is only 30, so we will also need to update it by using `tmap_options(max.categories = 55)` such that all planning zones are shown

```{r}
ttm() #tmap_mode("view") #alternative
tmap_options(check.and.fix = TRUE)
tmap_options(max.categories = 55)
```

We then create an interactive symbol map. The tm view() function's set.zoom.limits argument specifies a minimum and maximum zoom level of 11 and 14, respectively.

```{r}
tm_shape(mpsz_svy21) +
  tm_polygons("PLN_AREA_N") +
  
tm_shape(condo_resale_sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6, 
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

`ttm()` will be used to switch R toggle back tmap back to plot mode before proceeding on to the following section.

```{r}
ttm()
#tmap_mode("plot") #alternative
```

## Hedonic Pricing Modelling in R

In this section, we will learn how to develop Hedonic pricing models for condominium resale units using [`lm()`](https://www.rdocumentation.org/packages/stats/versions/3.5.2/topics/lm) of **R base**.

```{r}
condo_slr = lm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale_sf)
```

`lm()` returns an object of class "*lm*" or for multiple responses of class `c("mlm", "lm")`.

To acquire and print a summary and analysis of variance table of the results, we can use the functions `summary()` and `anova()`. The generic accessor functions coefficients, effects, fitted.values, and residuals take the value returned by *lm* and extract a number of important properties.

```{r}
summary(condo_slr)
```

-   The output report reveals that the SELLING_PRICE can be explained by using the formula:\
    $$y = - 258121.1 + 14719.0x$$

-   An estimated 45% of the resale prices may be explained by the basic regression model, according to the R-squared value of ***0.4518***.

    We will ***reject the null hypothesis*** that mean is a good estimator of SELLING PRICE because the p-value is substantially lower than 0.0001.

    Hence, we can conclude that basic linear regression model is a reliable predictor of *SELLING_PRICE*.

-   The estimations of the Intercept and *AREA SQM* both have p-values that are less than 0.001, according to the report's **Coefficients** section.

    The **null hypothesis that B0 and B1 are equal to 0 will therefore be rejected**. As a consequence, we can conclude that ***B0*** and ***B1*** are accurate parameter estimates.

To visualise the best fit curve on a scatterplot, we can incorporate `lm()` as a method function in ggplot's geometry with the code below.

```{r}
ggplot(data=condo_resale_sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

We can observe from the scatter plot that there are a few statistical outliers with relative high selling price

### Multiple Linear Regression Method

#### Visualising the relationships of the independent variables

It's crucial to make sure that the independent variables employed are not highly correlated with one another before creating a multiple regression model.

The quality of the model will be compromised if these highly correlated independent variables are unintentionally employed in the construction of a multi linear regression model.

In statistics, this phenomenon is referred to as multicollinearity.

Correlation matrix is commonly used to visualize the relationships between the independent variables. Beside the `pairs()` of R, there are many packages support the display of a correlation matrix. In this section, the [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package will be used. We will only be interested in the columns between 5 and 23

```{r}
#| eval: false
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

![](corrplot.png)

Rearranging the matrix is crucial for uncovering its hidden patterns and structures. Corrplot (parameter order) has four techniques with the names "AOE," "FPC," "hclust," and "alphabet."

AOE order is applied in the following code section. It uses [Michael Friendly's](https://www.datavis.ca/papers/corrgram.pdf) recommended method of employing the angular order of the eigenvectors to organize the variables.

According to Calkins (2005), variables that can be regarded as having a high degree of correlation are indicated by correlation coefficients with magnitudes between ± 0.7 and 1.0.

From the scatterplot matrix, it is clear that

-   ***FREEHOLD*** is highly correlated to ***LEASE_99YEAR***.

-   ***PROX_CHILDCARE*** is highly correlated to ***PROX_BUS_STOP***.

In view of this, we will exclude ***LEASE_99YEAR*** and ***PROX_CHILDCARE*** in the subsequent model building.

#### Building a Hedonic pricing model using multiple linear regression method

The code below using `lm()` to calibrate the multiple linear regression model.

```{r}
condo_mlr = lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale_sf)

summary(condo_mlr)
```

#### Preparing Publication Quality Table: olsrr method

With reference to the summary statistics of *condo_mlr*, it is clear that not all the independent variables are statistically significant. We will revised the model by removing the variables which are not.

We will then use the [*`ols_regress`*](https://www.rdocumentation.org/packages/olsrr/versions/0.5.3/topics/ols_regress)*`()`* function to create the model

```{r}
condo_mlr1 = lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL +
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale_sf)

ols_regress(condo_mlr1)
```

-   Looking at the model summary of the report: An estimated 64% of the resale prices may be explained by the multi linear regression model, according to the R-squared value of ***0.641***.

-   The null hypothesis is that there is **no relationship between the X variables (our parameter estimates in the above report) and the Y variable (the condo resale price).**

    From the ANOVA analysis, the p-value is highly significant, and hence we will reject the null hypothesis, and can conclude that our parameter estimates variables are accurate.

-   From the Parameter estimates of the report, the variable *PROX_CHILDCARE* is not statistically significant as it has a p-value of more than 0.05, thus we will not take this variable into consideration.

    With that we can derive our formula as follows:

    $$
    y = 519269.367 + 12855.123x_1 - 24165.851x_2 - 80585.330x_3 + 202711.249x_4 + 37328.669x_5 - 220400.480x_6 + 561964.278 x_7 + 148324.081x_8 - 189783.935x_9 - 232.943x_{10} + 118218.279x_{11} + 318802.330x_{12}
    $$

    $x_1$ = *AREA_SQM*, $x_2$ = *AGE*, $x_3$ = *PROX_CBD*, $x_4$ = *PROX_ELDERLYCARE*, $x_5$ = *PROX_URA_GROWTH_AREA*, $x_6$ = *PROX_MRT*, $x_7$ = *PROX_PARK*, $x_8$ = *PROX_PRIMARY_SCH*, $x_9$ = *PROX_SHOPPING_MALL*, $x_{10}$ = *NO_Of_UNITS*, $x_{11}$ = *FAMILY_FRIENDLY*, $x_{12}$ = *FREEHOLD*

-   From the above, we can conclude that the following 7 variables are positive correlated with the condo resale price (i.e. they tend to move in the same direction)  - *AREA_SQM, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_PARK, PROX_PRIMARY_SCH, FAMILY_FRIENDLY, FREEHOLD*

-   From the above, we can conclude that the following 5 variables are negatively correlated with the condo resale price (i.e. they tend to move in the opposite direction) - *AGE, PROX_CBD, PROX_MRT, PROX_SHOPPING_MALL, NO_Of_UNITS*

#### Preparing Publication Quality Table: gtsummary method

An elegant and flexible method for producing publication-quality summary tables in R is the [gtsummary](https://www.danieldsjoberg.com/gtsummary/) package. The [`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) function is used to generate a nicely prepared regression report using the code below.

```{r}
tbl_regression(condo_mlr1, intercept = TRUE)
```

Model statistics can be added to the report using the **gtsummary** package by either appending them to the report table with the function [`add_glance_table()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) or adding them as a table source note with the function [add_glance_source_note()](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html), as demonstrated in the code below.

```{r}
tbl_regression(condo_mlr, intercept = TRUE) %>%
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, AIC, statistic, p.value, sigma)
  )
```

For more customization options, refer to [Tutorial: tbl_regression](https://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)

#### Checking for multicolinearity

A superb R package designed specifically for OLS regression is being introduced in this section. It is known as [**olsrr**](https://olsrr.rsquaredacademy.com/). It offers a selection of really helpful techniques for improving multiple linear regression models, including:

-   comprehensive regression output

-   residual diagnostics

-   measures of influence

-   heteroskedasticity tests

-   collinearity diagnostics

-   model fit assessment

-   variable contribution assessment

-   variable selection procedures

In the code below, the [`ols_vif_tol()`](https://olsrr.rsquaredacademy.com/reference/ols_coll_diag.html) of **olsrr** package is used to test if there are sign of multicollinearity.

```{r}
ols_vif_tol(condo_mlr1)
```

From the result, the independent variables' VIF are **smaller than 10**, therefore, It is safe to conclude that none of the independent variables show any evidence of multicollinearity.

#### Test for Non-Linearity

It is pertinent to verify the linearity and additivity of the relationship between the dependent and independent variables in multiple linear regression.

In the code below, the [`ols_plot_resid_fit()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html) function of **olsrr** package is used to conduct the linearity assumption test.

```{r}
ols_plot_resid_fit(condo_mlr1)
```

Since the majority of the data points are clustered around the 0 line in the resultant graph above, we can confidently infer that the dependent variable and independent variables have linear relationships.

#### Test for Normality Assumption

Before doing certain statistical tests or regression, you should ensure that the data generally matches a bell-shaped curve. This is known as the assumption of normality.

The code below uses [`ols_plot_resid_hist()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_hist.html) of **olsrr** package to perform the normality assumption test.

```{r}
ols_plot_resid_hist(condo_mlr1)
```

The result shows that the residual of the multiple linear regression model (i.e. `condo_mlr1`) matches a normal distribution. As demonstrated in the code below, the `ols_test_normality()` function of the **olsrr** package can be used if one prefer formal statistical test methods.

```{r}
ols_test_normality(condo_mlr1)
```

The p-values for the four tests are less than the alpha value of 0.05, as shown in the summary table above. Therefore, we will conclude that the residuals are not normally distributed and **reject the null hypothesis.**

#### Testing for Spatial Autocorrelation

Since the Hedonic model we attempt to construct uses attributes that are geographically referenced, it is important for us to see the residual of the Hedonic pricing model.

*condo_resale_sf* from the SF data frame needs to be transformed into a *SpatialPointsDataFrame* in order to run the spatial autocorrelation test.

1.  First, export the residual of the Hedonic pricing model and save it as a data frame using `as.data.frame()`

    ```{r}
    mlr_output = as.data.frame(condo_mlr1$residuals)
    ```

2.  Join the newly created data frame with *condo_resale_sf* object with `cbind()`.

    ```{r}
    condo_resale_res_sf = cbind(condo_resale_sf, 
                            mlr_output) %>%
                rename(`MLR_RES` = `condo_mlr1.residuals`)
    ```

3.  Because the **spdep** package can only process sp compliant spatial data objects, we need to transform *condo_resale_res_sf* from a simple feature object into a *SpatialPointsDataFrame* with `as_spatial()`

    ```{r}
    condo_resale_sp = as_Spatial(condo_resale_res_sf)
    ```

4.  We will use **tmap** package to display the distribution of the residuals on an interactive map

    ```{r}
    ttm()

    symbolmap = tm_shape(mpsz_svy21) +
                tmap_options(check.and.fix = TRUE) +
                tm_polygons("PLN_AREA_N", alpha = 0.5) + 
                tm_shape(condo_resale_res_sf) +  
                tm_dots(col = "MLR_RES",
                        alpha = 0.8,
                        style="quantile") +
                tm_view(set.zoom.limits = c(11,14))

    symbolmap
    ttm()
    ```

There are indications of spatial autocorrelation, as shown by the map above. The Moran's I test can be use to verify if our observation is accurate.

#### Moran's I test for spatial autocorrelation

We will create the distance base matrix by using dnearneigh() function of **spdep**.

```{r}
nb = dnearneigh(coordinates(condo_resale_sp), 0, 1500, longlat = FALSE)
summary(nb)
```

The resulting neighbours lists (nb) will then be transformed into spatial weights using the `nb2listw()` function of the **spdep** packge.

```{r}
nb_lw = nb2listw(nb, style = 'W')
summary(nb_lw)
```

**The null hypothesis**

> *The null hypothesis in this case is that residuals are randomly distributed.*

[`lm.morantest()`](https://r-spatial.github.io/spdep/reference/lm.morantest.html) of **spdep** package will be used to perform Moran's I test for residual spatial autocorrelation

```{r}
lm.morantest(condo_mlr1, nb_lw)
```

Based on the result, we will ***reject*** the null hypothesis as the p-value is less than 0.05. In fact as the p-value is less than 0.01, we can consider that as **highly significant.**

Therefore, we can conclude that the residuals is not randomly distributed based on Moran's I statistics.

As the observed Global Moran I is 0.1438876, which is more than 0, we can then also conclude that the residuals follows a cluster distribution.

## Building Hedonic Pricing Models using GW model

We will be introduced to model Hedonic pricing in this section using both ***fixed*** and ***adaptive*** bandwidth strategies.

### Building Fixed Bandwidth GWR Model

#### Computing Fixed Bandwidth

The optimal fixed bandwidth to employ in the model is chosen using the `bw.gwr()` function of the **GWModel** package, which is utilized in the code block below.

The argument *adaptive* is set to FALSE, indicating that we want to compute the fixed bandwidth

The ***CV cross-validation*** approach and the ***AIC corrected (AICc)*** approach are the two methods that could be used to identify the stopping rule. We use the *approach* argument to define the stopping rule.

##### ***CV cross-validation Approach - FIXED*** Bandwidth

```{r}
bw.fixed.cv = bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,                   data=condo_resale_sp,
                  approach = "CV", 
                  kernel = "gaussian",
                  adaptive = FALSE, 
                  longlat = FALSE
                  )
```

The result shows that the recommended bandwidth is 1007.229 metres. The reason why it is in meters is due to the fact that the projection system SYV21 measures distance in meters.

##### GWModel method - ***CV cross-validation Approach - FIXED*** Bandwidth

Now, utilizing fixed bandwidth and a Gaussian kernel, we may calibrate the ***gwr*** ***model*** using the code below.

```{r}
gwr.fixed.cv = gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale_sp, 
                      bw=bw.fixed.cv,
                      kernel = "gaussian",
                      longlat = FALSE
                      )

gwr.fixed.cv
```

The report shows that the adjusted $R^2$ of the ***gwr*** is 0.8306, which is significantly better than the global multiple linear regression model of 0.6411.

The AICC Value in this case is 42298.78

##### ***AIC corrected (AICc) Approach - FIXED Bandwidth***

We use the same function`bw.gwr()` here, except that we will change the approach to *AICc*

```{r}
bw.fixed.aicc = bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,                   data=condo_resale_sp,
                  approach = "AICc", 
                  kernel = "gaussian",
                  adaptive = FALSE, 
                  longlat = FALSE
                  )
```

The result shows that the recommended bandwidth is 1194.819 metres.

##### GWModel method - ***AIC corrected (AICc) Approach - FIXED Bandwidth***

Now, utilizing fixed bandwidth and a Gaussian kernel, we may calibrate the ***gwr*** ***model*** using the code below.

```{r}
gwr.fixed.aicc = gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale_sp, 
                      bw=bw.fixed.aicc,
                      kernel = "gaussian",
                      longlat = FALSE
                      )

gwr.fixed.aicc
```

The report shows that the adjusted $R^2$ of the ***gwr*** is 0.8192, which is significantly better than the global multiple linear regression model of 0.6411.

The AICC Value in this case is 42291.95.

##### Comparison of the 2 approaches for FIXED Bandwidth

Comparing the AICC value of using the CV Approach (42298.78) vs the AICC Approach (42291.95), the difference between the 2 method differs by 6.83, which is greater than 3. Hence we can conclude that the AICC Approach for fixed bandwidth is a better model

### Building Adaptive Bandwidth GWR Model

In this section, we'll use an adaptive bandwidth approach to calibrate the gwr-based Hedonic pricing model.

#### Computing the adaptive bandwidth

Similar to the earlier section, we will first use `bw.ger()` to determine the recommended data point to use.

The code used look very similar to the one used to compute the fixed bandwidth except the **adaptive argument has changed to TRUE**.

##### CV cross-validation Approach - ADAPTIVE Bandwidth

```{r}
bw.adaptive.cv = bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                   
                  data=condo_resale_sp,
                  approach = "CV", 
                  kernel = "gaussian",
                  adaptive = TRUE, 
                  longlat = FALSE
                  )
```

The outcome demonstrates that 30 data points should be used.

##### GWModel method - CV cross-validation Approach - ADAPTIVE Bandwidth

Now, utilizing fixed bandwidth and a Gaussian kernel, we may calibrate the ***gwr*** ***model*** using the code below.

```{r}
gwr.adaptive.cv <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
PROX_PRIMARY_SCH + PROX_SHOPPING_MALL +                 NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale_sp, 
                          bw=bw.adaptive.cv, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)

gwr.adaptive.cv
```

The report shows that the adjusted $R^2$ of the ***gwr*** is 0.845 which is significantly better than the global multiple linear regression model of 0.6411.

The AICC Value of this approach is 42058.25.

##### ***AIC corrected (AICc) Approach - ADAPTIVE Bandwidth***

We use the same function`bw.gwr()` here, except that we will change the approach to *AICc*

```{r}
bw.adaptive.aicc = bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                   
                  data=condo_resale_sp,
                  approach = "AICc", 
                  kernel = "gaussian",
                  adaptive = TRUE, 
                  longlat = FALSE
                  )
```

The outcome demonstrates that 27 data points should be used.

##### GWModel method - AIC corrected (AICc) Approach - ADAPTIVE Bandwidth

Now, utilizing fixed bandwidth and a Gaussian kernel, we may calibrate the ***gwr*** ***model*** using the code below.

```{r}
gwr.adaptive.aicc <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
PROX_PRIMARY_SCH + PROX_SHOPPING_MALL +                 NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale_sp, 
                          bw=bw.adaptive.aicc, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)

gwr.adaptive.aicc
```

The report shows that the adjusted $R^2$ of the ***gwr*** is 0.8522, which is significantly better than the global multiple linear regression model of 0.6411.

The AICC Value in this case is 42054.2

##### Comparison of the 2 approaches for ADAPTIVE Bandwidth

Comparing the AICC value of using the CV Approach (42058.25) vs the AICC Approach (42054.2), the difference between the 2 method differs by 4.05, which is greater than 3. Hence we can conclude that the AICC Approach for adaptive bandwidth is a better model.

#### Comparing FIXED & ADAPTIVE bandwidth 

The results of our tests shows that the AICC Approach produces a better model for both *fixed* and *adaptive* bandwidth as they both produce lower AICC value greater than 3.

With the AICC Approach, Adaptive bandwidth produces a higher $R^2$ of 0.8522 vs the fixed bandwidth $R^2$ of 0.8192. Additionally, adaptive bandwidth method produces higher a AICC value as compared to the fixed bandwidth method, with a difference of 237.75 (AICC Value: 42054.2 vs. 42291.95).

Consequently, moving forward, we will apply the AICC Approach with Adaptive bandwidth.

### Visualizing GWR Output

The output feature class table has fields for observed and predicted y values, condition number (cond), local $R^2$ , residuals, and explanatory variable coefficients and standard errors in addition to regression residuals:

-   Condition Number: local collinearity is evaluated by this diagnostic. Results become unstable in the presence of high local collinearity. Results for condition numbers greater than 30 might not be accurate.

-   Local $R^2$: these numbers, which range from 0.0 to 1.0, show how well the local regression model fits the observed y values. Very low numbers suggest that the local model is not performing well. It may be possible to identify important variables that may be missing from the regression model by mapping the Local R2 values to observe where GWR predicts well and poorly.

-   Predicted: these are the estimated (or fitted) y values 3. computed by GWR.

-   Residuals: To get the residual values, subtract the fitted y values from the actual y values. Standardized residuals have a **mean of 0** and a **standard deviation of 1**. Using these numbers, a cold-to-hot rendered map of standardized residuals can be generated.

-   Coefficient Standard Error: These numbers reflect the reliability of each coefficient estimate. When standard errors are low relative to the actual coefficient values, the confidence in those estimations is increased. Problems with local collinearity may be indicated by large standard errors.

They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its "data" slot in an object called **SDF** of the output list.

### Converting SDF into *sf* data.frame

To visualize the fields in **SDF**, we need to first convert it into **sf** data.frame by using `st_as_sf()` function and then transforming it to SVY21 projection with `st_transform()` using the crs=3414

```{r}
condo_resale_sf_adaptive = st_as_sf(gwr.adaptive.aicc$SDF) %>%
                           st_transform(crs=3414)

gwr_adaptive_output = as.data.frame(gwr.adaptive.aicc$SDF)
condo_resale_sf_adaptive = cbind(condo_resale_res_sf, 
                           as.matrix(gwr_adaptive_output))
```

We can use `glimpse()` and summary statistics to examine the content of *condo_resale_sf_adaptive* sf data frame.

```{r}
glimpse(condo_resale_sf_adaptive)
summary(gwr.adaptive.aicc$SDF$yhat)
```

### Visualizing local R2

We can create an interactive point symbol map using **tmap** functions. We toggle the mode to interactive mode with `ttm()`

```{r}
ttm()
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons("PLN_AREA_N", alpha = 0.1) +
  
tm_shape(condo_resale_sf_adaptive) +  
  tm_dots(col = "Local_R2",
          palette="PuRd",
          border.col = "gray50",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,15))
```

From the map above, we can tell that the GWR model will accurately predict the condo resale prices most of the time as they have adjusted $R^2$ value of at least 0.8. The areas of Bishan, Novena and Kallang may fall short in comparison as $R^2$ falls below 80%.

#### By URA Planning Region

If we are interested in just the central region, we could plot the map below by filtering at the `tm_shape` level

```{r}
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons("PLN_AREA_N", alpha = 0.1) +
  
tm_shape(condo_resale_sf_adaptive) + 
  tm_bubbles(col = "Local_R2",
              palette="PuRd",
              size = 0.15,
              border.col = "gray50",
              border.lwd = 1) +
  tm_view(set.zoom.limits = c(12,15))
```

### Visualizing coefficient estimates

#### Computing p-values from t-values

We can compute the p value from the t-value by using the `pt()` function in R (Zach (2020)). In this case, we will conduct a 2 tail test of significance and set the degree of freedom to the number of rows in the *condo_resale_sf_adaptive* data frame.

> *The null hypothesis is that the parameter variables does not affect the condo resale price.*

We can add a new column *AREA_SQM_PV* by using the `mutate()` function to store the p-value

```{r}
condo_resale_sf_adaptive = condo_resale_sf_adaptive %>%
  mutate(`AREA_SQM_PV` = 2*pt(q=condo_resale_sf_adaptive$AREA_SQM_TV, df=nrow(condo_resale_sf_adaptive), lower.tail=FALSE))
```

#### Plotting the map with Planning area and subzones with p-value and standard error

Once we find the p-value, we could used it to create an interactive point symbol map, with fixed break denoting significance level up to 0.05.

```{r}

AREA_SQM_SE = 
tm_shape(mpsz)+
  tm_polygons("PLN_AREA_N", alpha = 0.1) +

tm_shape(mpsz_svy21) +
  tm_polygons("SUBZONE_N", alpha = 0.2) +  
  
tm_shape(condo_resale_sf_adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          palette="PuRd",
          border.col = "gray50",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,15))

AREA_SQM_TV = 
tm_shape(mpsz)+
  tm_polygons("PLN_AREA_N", alpha = 0.1) +

tm_shape(mpsz_svy21) +
  tm_polygons("SUBZONE_N", alpha = 0.2) +  
  
tm_shape(condo_resale_sf_adaptive) +  
  tm_dots(col = "AREA_SQM_PV",
          palette="-PuRd",
          border.col = "gray50",
          breaks=c(-Inf, 0.001, 0.01, 0.05,  Inf),
          style="fixed",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,15))

tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=1, nrow=2,
             sync = TRUE)
```

We toggle the mode back to plotting with `ttm()`

```{r}
ttm()
```

#### Analysis of Results

From the p-value map, we can see that most of the points are statistically significant, except for the following areas:

-   Ang Mo Kio (Yio Chu Kang West)

-   Bedok (East of Bedok Near ECP)

-   Mandai (Mandai Estate)

-   Sembawang (Admiralty, Sembawang Spring)

-   Tampines (North of Simei near PIE)

-   Woodlands (Woodgrove), this area of interest, however have a high standard error which will increase the p-value causing it to be statistically insignificant. This could be due to high variance in the condo resale price for this subzone in this data set.

-   Jurong West (Hong Kah, Taman Jurong)

-   Yishun (Yishun West)

We can observe that 2 areas near expressways might affect the condo resale prices. We could further investigate and add proximity to expressways as a parameter variable if it is available.

## References

Calkins K. G (2005) *Applied Statistics - Lesson 5, Correlation Coefficients*

https://www.andrews.edu/\~calkins/math/edrm611/edrm05.htm#:\~:text=Correlation%20coefficients%20whose%20magnitude%20are,can%20be%20considered%20highly%20correlated.

Zach (2020), *How to Calculate the P-Value of a T-Score in R*

https://www.statology.org/p-value-of-t-score-r/
